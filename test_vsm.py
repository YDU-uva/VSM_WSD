#!/usr/bin/env python3
"""
VSMæ¨¡å‹çš„ç®€å•æµ‹è¯•è„šæœ¬
éªŒè¯æ¨¡å‹ç»„ä»¶æ˜¯å¦æ­£å¸¸å·¥ä½œ
"""

import torch
import torch.nn.functional as F
import numpy as np
from models.vsm_models import VariationalEncoder, SemanticMemoryModule, VariationalDecoder, VSMModel

def test_variational_encoder():
    """æµ‹è¯•å˜åˆ†ç¼–ç å™¨"""
    print("æµ‹è¯•å˜åˆ†ç¼–ç å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, input_dim = 2, 10, 300
    hidden_dim, latent_dim = 256, 128
    
    encoder = VariationalEncoder(input_dim, hidden_dim, latent_dim)
    x = torch.randn(batch_size, seq_len, input_dim)
    
    # å‰å‘ä¼ æ’­
    mu, logvar, z = encoder(x)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert mu.shape == (batch_size, seq_len, latent_dim), f"mu shape error: {mu.shape}"
    assert logvar.shape == (batch_size, seq_len, latent_dim), f"logvar shape error: {logvar.shape}"
    assert z.shape == (batch_size, seq_len, latent_dim), f"z shape error: {z.shape}"
    
    # æ£€æŸ¥é‡å‚æ•°åŒ–æ€§è´¨
    mu2, logvar2, z2 = encoder(x)
    assert not torch.equal(z, z2), "é‡å‚æ•°åŒ–åº”è¯¥äº§ç”Ÿä¸åŒçš„é‡‡æ ·"
    assert torch.equal(mu, mu2), "å‡å€¼åº”è¯¥ä¸€è‡´"
    assert torch.equal(logvar, logvar2), "æ–¹å·®åº”è¯¥ä¸€è‡´"
    
    print("âœ“ å˜åˆ†ç¼–ç å™¨æµ‹è¯•é€šè¿‡")


def test_semantic_memory():
    """æµ‹è¯•è¯­ä¹‰è®°å¿†æ¨¡å—"""
    print("æµ‹è¯•è¯­ä¹‰è®°å¿†æ¨¡å—...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, latent_dim = 2, 10, 128
    memory_size = 64
    
    memory = SemanticMemoryModule(memory_size, latent_dim)
    z = torch.randn(batch_size, seq_len, latent_dim)
    
    # å‰å‘ä¼ æ’­
    attended_memory, attention_weights = memory(z)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert attended_memory.shape == (batch_size, seq_len, latent_dim), f"attended_memory shape error: {attended_memory.shape}"
    assert attention_weights.shape == (batch_size, seq_len, memory_size), f"attention_weights shape error: {attention_weights.shape}"
    
    # æ£€æŸ¥æ³¨æ„åŠ›æƒé‡å½’ä¸€åŒ–
    attention_sum = attention_weights.sum(dim=-1)
    assert torch.allclose(attention_sum, torch.ones_like(attention_sum), atol=1e-6), "æ³¨æ„åŠ›æƒé‡åº”è¯¥å½’ä¸€åŒ–"
    
    # æµ‹è¯•è®°å¿†æ›´æ–°
    memory_before = memory.memory.data.clone()
    memory.train()
    attended_memory2, _ = memory(z, update_memory=True)
    memory_after = memory.memory.data
    
    # è®°å¿†åº”è¯¥æœ‰æ‰€æ›´æ–°
    assert not torch.equal(memory_before, memory_after), "è®°å¿†åº”è¯¥åœ¨è®­ç»ƒæ—¶æ›´æ–°"
    
    print("âœ“ è¯­ä¹‰è®°å¿†æ¨¡å—æµ‹è¯•é€šè¿‡")


def test_variational_decoder():
    """æµ‹è¯•å˜åˆ†è§£ç å™¨"""
    print("æµ‹è¯•å˜åˆ†è§£ç å™¨...")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len, latent_dim = 2, 10, 128
    hidden_dim, output_dim = 256, 64
    
    decoder = VariationalDecoder(latent_dim, hidden_dim, output_dim)
    z = torch.randn(batch_size, seq_len, latent_dim)
    
    # å‰å‘ä¼ æ’­
    output = decoder(z)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    assert output.shape == (batch_size, seq_len, output_dim), f"output shape error: {output.shape}"
    
    print("âœ“ å˜åˆ†è§£ç å™¨æµ‹è¯•é€šè¿‡")


def test_vsm_model():
    """æµ‹è¯•å®Œæ•´çš„VSMæ¨¡å‹"""
    print("æµ‹è¯•å®Œæ•´VSMæ¨¡å‹...")
    
    # åˆ›å»ºæ¨¡å‹å‚æ•°
    model_params = {
        'embed_dim': 300,
        'hidden_size': 256,
        'latent_dim': 128,
        'memory_size': 64,
        'beta': 1.0,
        'dropout_ratio': 0.1
    }
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size, seq_len = 2, 10
    input_len = [8, 6]  # ä¸åŒçš„åºåˆ—é•¿åº¦
    
    vsm = VSMModel(model_params)
    x = torch.randn(batch_size, seq_len, model_params['embed_dim'])
    
    # å‰å‘ä¼ æ’­
    output, losses = vsm(x, input_len, compute_loss=True)
    
    # æ£€æŸ¥è¾“å‡ºå½¢çŠ¶
    expected_output_dim = model_params['hidden_size'] // 4
    assert output.shape == (batch_size, seq_len, expected_output_dim), f"output shape error: {output.shape}"
    
    # æ£€æŸ¥æŸå¤±
    assert 'kl_loss' in losses, "åº”è¯¥åŒ…å«KLæŸå¤±"
    assert 'recon_loss' in losses, "åº”è¯¥åŒ…å«é‡æ„æŸå¤±"
    assert 'memory_reg_loss' in losses, "åº”è¯¥åŒ…å«è®°å¿†æ­£åˆ™åŒ–æŸå¤±"
    assert 'total_loss' in losses, "åº”è¯¥åŒ…å«æ€»æŸå¤±"
    
    # æ£€æŸ¥æŸå¤±å€¼çš„åˆç†æ€§
    for loss_name, loss_value in losses.items():
        assert not torch.isnan(loss_value), f"{loss_name} ä¸åº”è¯¥æ˜¯NaN"
        assert loss_value.item() >= 0, f"{loss_name} åº”è¯¥éè´Ÿ"
    
    # æµ‹è¯•æ— æŸå¤±è®¡ç®—
    output_no_loss, losses_no_loss = vsm(x, input_len, compute_loss=False)
    assert losses_no_loss == {}, "ä¸è®¡ç®—æŸå¤±æ—¶åº”è¯¥è¿”å›ç©ºå­—å…¸"
    assert torch.equal(output, output_no_loss), "è¾“å‡ºåº”è¯¥ä¸€è‡´"
    
    print("âœ“ å®Œæ•´VSMæ¨¡å‹æµ‹è¯•é€šè¿‡")


def test_attention_analysis():
    """æµ‹è¯•æ³¨æ„åŠ›æƒé‡åˆ†æ"""
    print("æµ‹è¯•æ³¨æ„åŠ›æƒé‡åˆ†æ...")
    
    model_params = {
        'embed_dim': 300,
        'hidden_size': 256,
        'latent_dim': 128,
        'memory_size': 64,
        'beta': 1.0,
        'dropout_ratio': 0.1
    }
    
    batch_size, seq_len = 2, 10
    input_len = [8, 6]
    
    vsm = VSMModel(model_params)
    x = torch.randn(batch_size, seq_len, model_params['embed_dim'])
    
    # è·å–æ³¨æ„åŠ›æƒé‡
    attention_weights = vsm.get_attention_weights(x, input_len)
    
    # æ£€æŸ¥å½¢çŠ¶
    assert attention_weights.shape == (batch_size, seq_len, model_params['memory_size'])
    
    # æ£€æŸ¥æƒé‡å½’ä¸€åŒ–
    attention_sum = attention_weights.sum(dim=-1)
    assert torch.allclose(attention_sum, torch.ones_like(attention_sum), atol=1e-6)
    
    print("âœ“ æ³¨æ„åŠ›æƒé‡åˆ†ææµ‹è¯•é€šè¿‡")


def test_memory_regularization():
    """æµ‹è¯•è®°å¿†æ­£åˆ™åŒ–"""
    print("æµ‹è¯•è®°å¿†æ­£åˆ™åŒ–...")
    
    # åˆ›å»ºæç«¯ç›¸ä¼¼çš„è®°å¿†
    memory_size, latent_dim = 8, 16
    memory = SemanticMemoryModule(memory_size, latent_dim)
    
    # è®¾ç½®æ‰€æœ‰è®°å¿†å‘é‡ç›¸åŒ
    memory.memory.data.fill_(1.0)
    
    model_params = {
        'embed_dim': latent_dim,
        'hidden_size': 64,
        'latent_dim': latent_dim,
        'memory_size': memory_size,
        'beta': 1.0
    }
    
    vsm = VSMModel(model_params)
    vsm.memory = memory
    
    # è®¡ç®—æ­£åˆ™åŒ–æŸå¤±
    reg_loss = vsm._compute_memory_regularization()
    
    # ç›¸åŒçš„å‘é‡åº”è¯¥äº§ç”Ÿé«˜æ­£åˆ™åŒ–æŸå¤±
    assert reg_loss.item() > 0.5, f"ç›¸åŒè®°å¿†å‘é‡çš„æ­£åˆ™åŒ–æŸå¤±åº”è¯¥è¾ƒé«˜ï¼Œå®é™…: {reg_loss.item()}"
    
    # è®¾ç½®æ­£äº¤è®°å¿†å‘é‡
    memory.memory.data = torch.eye(memory_size, latent_dim)
    reg_loss_ortho = vsm._compute_memory_regularization()
    
    # æ­£äº¤å‘é‡åº”è¯¥äº§ç”Ÿæ›´ä½çš„æ­£åˆ™åŒ–æŸå¤±
    assert reg_loss_ortho.item() < reg_loss.item(), "æ­£äº¤å‘é‡çš„æ­£åˆ™åŒ–æŸå¤±åº”è¯¥æ›´ä½"
    
    print("âœ“ è®°å¿†æ­£åˆ™åŒ–æµ‹è¯•é€šè¿‡")


def main():
    """è¿è¡Œæ‰€æœ‰æµ‹è¯•"""
    print("å¼€å§‹VSMæ¨¡å‹æµ‹è¯•...")
    print("=" * 50)
    
    try:
        test_variational_encoder()
        test_semantic_memory()
        test_variational_decoder()
        test_vsm_model()
        test_attention_analysis()
        test_memory_regularization()
        
        print("=" * 50)
        print("ğŸ‰ æ‰€æœ‰æµ‹è¯•é€šè¿‡ï¼VSMæ¨¡å‹å®ç°æ­£ç¡®ã€‚")
        
    except Exception as e:
        print(f"âŒ æµ‹è¯•å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False
    
    return True


if __name__ == '__main__':
    main() 